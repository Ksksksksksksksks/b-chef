{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13444726,"sourceType":"datasetVersion","datasetId":8534007}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video-Model for B-Chef\n\nIn this notebool we will ~~try to~~ train own version of pre-trained model, called SlowFast","metadata":{}},{"cell_type":"markdown","source":"## Imports and defininigs","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio pytorchvideo --quiet\n!pip install omegaconf einops tqdm --quiet\n!apt install p7zip-full --quiet\n\nimport os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:33:10.622608Z","iopub.execute_input":"2025-10-20T16:33:10.622921Z","iopub.status.idle":"2025-10-20T16:35:08.871176Z","shell.execute_reply.started":"2025-10-20T16:33:10.622896Z","shell.execute_reply":"2025-10-20T16:35:08.870162Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mReading package lists...\nBuilding dependency tree...\nReading state information...\np7zip-full is already the newest version (16.02+dfsg-8).\n0 upgraded, 0 newly installed, 0 to remove and 132 not upgraded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:42:45.474288Z","iopub.execute_input":"2025-10-20T18:42:45.474708Z","iopub.status.idle":"2025-10-20T18:42:45.479951Z","shell.execute_reply.started":"2025-10-20T18:42:45.474681Z","shell.execute_reply":"2025-10-20T18:42:45.479154Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!pip install mlflow --quiet\nimport mlflow\nimport mlflow.pytorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:01:14.219616Z","iopub.execute_input":"2025-10-20T18:01:14.220323Z","iopub.status.idle":"2025-10-20T18:02:01.075722Z","shell.execute_reply.started":"2025-10-20T18:01:14.220295Z","shell.execute_reply":"2025-10-20T18:02:01.074864Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m749.8/749.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"BASE_DIR = \"/kaggle/input/b-chef-tensors\"\n\n# CSV with metadata\nTRAIN_CSV = os.path.join(BASE_DIR, \"train.csv\")\nVAL_CSV   = os.path.join(BASE_DIR, \"val.csv\")\nTEST_CSV  = os.path.join(BASE_DIR, \"test.csv\")\nLABELS_SUMMARY = os.path.join(BASE_DIR, \"labels_summary.csv\")\n\n# folders with tesors\nTENSORS_DIR = os.path.join(BASE_DIR, \"tensors\", \"tensors\")\nTRAIN_DIR = os.path.join(TENSORS_DIR, \"train\")\nVAL_DIR   = os.path.join(TENSORS_DIR, \"val\")\nTEST_DIR  = os.path.join(TENSORS_DIR, \"test\")\n\n# check\nprint(\"Train tensors:\", os.listdir(TRAIN_DIR)[:5])\nprint(\"Val tensors:\", os.listdir(VAL_DIR)[:5])\nprint(\"Test tensors:\", os.listdir(TEST_DIR)[:5])\nprint(\"Train CSV exists:\", os.path.exists(TRAIN_CSV))\nprint(\"Labels summary exists:\", os.path.exists(LABELS_SUMMARY))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:44:35.873632Z","iopub.execute_input":"2025-10-20T16:44:35.874406Z","iopub.status.idle":"2025-10-20T16:44:35.908134Z","shell.execute_reply.started":"2025-10-20T16:44:35.874359Z","shell.execute_reply":"2025-10-20T16:44:35.907185Z"}},"outputs":[{"name":"stdout","text":"Train tensors: ['194_cut_slices.pt', '56_peel.pt', '241_peel.pt', '14_put_on_cutting-board.pt', '359_stir.pt']\nVal tensors: ['65_put_on_plate.pt', '87_cut_slices.pt', '14_put_on_cutting-board.pt', '86_pour.pt', '49_cut_apart.pt']\nTest tensors: ['14_cut_apart.pt', '31_pour.pt', '56_peel.pt', '11_peel.pt', '92_pour.pt']\nTrain CSV exists: True\nLabels summary exists: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:14:21.542616Z","iopub.execute_input":"2025-10-20T17:14:21.543051Z","iopub.status.idle":"2025-10-20T17:14:21.551119Z","shell.execute_reply.started":"2025-10-20T17:14:21.543027Z","shell.execute_reply":"2025-10-20T17:14:21.549979Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Work wth data","metadata":{}},{"cell_type":"markdown","source":"### Check that everything is accessible","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\ntest_df  = pd.read_csv(TEST_CSV)\nlabels_summary = pd.read_csv(LABELS_SUMMARY)\n\nprint(\"Train CSV sample:\")\ndisplay(train_df.head())\n\nprint(\"Labels summary:\")\ndisplay(labels_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:45:19.596760Z","iopub.execute_input":"2025-10-20T16:45:19.597154Z","iopub.status.idle":"2025-10-20T16:45:19.694895Z","shell.execute_reply.started":"2025-10-20T16:45:19.597128Z","shell.execute_reply":"2025-10-20T16:45:19.693928Z"}},"outputs":[{"name":"stdout","text":"Train CSV sample:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                          filename         label\n0  s10-d02-cam-002_23807_23844.avi   put_in_bowl\n1    s10-d02-cam-002_7280_8569.avi          peel\n2    s12-d10-cam-002_9017_9073.avi     cut_apart\n3    s11-d12-cam-002_9131_9323.avi  put_on_plate\n4    s16-d06-cam-002_9363_9547.avi          pour","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>s10-d02-cam-002_23807_23844.avi</td>\n      <td>put_in_bowl</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>s10-d02-cam-002_7280_8569.avi</td>\n      <td>peel</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>s12-d10-cam-002_9017_9073.avi</td>\n      <td>cut_apart</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>s11-d12-cam-002_9131_9323.avi</td>\n      <td>put_on_plate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>s16-d06-cam-002_9363_9547.avi</td>\n      <td>pour</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Labels summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                   label  count\n0              cut_apart     75\n1               cut_dice     75\n2             cut_slices     74\n3                  grate      5\n4                    mix      8\n5               open_egg     14\n6                   peel     75\n7                   pour     75\n8            put_in_bowl     73\n9         put_in_pan-pot     58\n10  put_on_cutting-board     75\n11          put_on_plate     75\n12                  stir     75","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cut_apart</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cut_dice</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cut_slices</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>grate</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mix</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>open_egg</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>peel</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>pour</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>put_in_bowl</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>put_in_pan-pot</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>put_on_cutting-board</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>put_on_plate</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>stir</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"sample_file = os.listdir(TRAIN_DIR)[0]\nsample_path = os.path.join(TRAIN_DIR, sample_file)\nprint(\"Loading:\", sample_file)\n\ntensor, label = torch.load(sample_path)\nprint(\"Tensor shape:\", tensor.shape)\nprint(\"Label:\", label)\n# 32 frames, 3 channels (RGB), each frame size 224x224","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:55:24.045085Z","iopub.execute_input":"2025-10-20T16:55:24.045501Z","iopub.status.idle":"2025-10-20T16:55:24.082476Z","shell.execute_reply.started":"2025-10-20T16:55:24.045473Z","shell.execute_reply":"2025-10-20T16:55:24.081559Z"}},"outputs":[{"name":"stdout","text":"Loading: 194_cut_slices.pt\nTensor shape: torch.Size([32, 3, 224, 224])\nLabel: cut_slices\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Creating TensorVideoDataset\n\nIt is class that basically do the cell above for all files, but more convenient","metadata":{}},{"cell_type":"code","source":"class TensorVideoDataset(Dataset):\n    def __init__(self, tensor_dir, csv_path, encoder=None, fit_encoder=False):\n        self.tensor_dir = tensor_dir\n        self.files = [f for f in os.listdir(tensor_dir) if f.endswith(\".pt\")]\n        self.df = pd.read_csv(csv_path) if csv_path else None\n        self.encoder = encoder\n        if fit_encoder:\n            labels = [f.split(\"_\", 1)[1].replace(\".pt\", \"\") for f in self.files]\n            self.encoder = LabelEncoder()\n            self.encoder.fit(labels)\n\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.tensor_dir, self.files[idx])\n        video_tensor, _ = torch.load(path)\n\n        label_name = os.path.basename(path).split(\"_\", 1)[1].replace(\".pt\", \"\")\n\n        if self.encoder:\n            label = torch.tensor(self.encoder.transform([label_name])[0])\n        else:\n            label = label_name\n\n        return video_tensor, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:13:06.619315Z","iopub.execute_input":"2025-10-20T19:13:06.620244Z","iopub.status.idle":"2025-10-20T19:13:06.628329Z","shell.execute_reply.started":"2025-10-20T19:13:06.620213Z","shell.execute_reply":"2025-10-20T19:13:06.627292Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# creating datasets\ntrain_dataset = TensorVideoDataset(\n    tensor_dir=TRAIN_DIR,\n    csv_path=TRAIN_CSV,\n    fit_encoder=True\n)\n\nval_dataset = TensorVideoDataset(\n    tensor_dir=VAL_DIR,\n    csv_path=VAL_CSV,\n    encoder=train_dataset.encoder  # use same encoder\n)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples:   {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:13:11.422252Z","iopub.execute_input":"2025-10-20T19:13:11.423617Z","iopub.status.idle":"2025-10-20T19:13:11.469706Z","shell.execute_reply.started":"2025-10-20T19:13:11.423568Z","shell.execute_reply":"2025-10-20T19:13:11.468804Z"}},"outputs":[{"name":"stdout","text":"Train samples: 529\nVal samples:   114\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"### Create dataloaders\n\nFor batches, shuffling & iterations","metadata":{}},{"cell_type":"code","source":"def pad_collate(batch):\n    # batch = [(video_tensor, label), ...]\n\n    max_len = max(v.shape[0] for v, _ in batch)\n    \n    videos = []\n    labels = []\n    \n    for v, l in batch:\n        if v.shape[0] < max_len:\n            pad = torch.zeros((max_len - v.shape[0], *v.shape[1:]))\n            v = torch.cat([v, pad], dim=0)\n        videos.append(v)\n        labels.append(l)\n    \n    videos = torch.stack(videos)\n    labels = torch.tensor(labels)\n    \n    return videos, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:45:55.898826Z","iopub.execute_input":"2025-10-20T19:45:55.899802Z","iopub.status.idle":"2025-10-20T19:45:55.906780Z","shell.execute_reply.started":"2025-10-20T19:45:55.899772Z","shell.execute_reply":"2025-10-20T19:45:55.905695Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"BATCH_SIZE = 8 if device.type == \"cuda\" else 2\nprint(\"Batch size set to:\", BATCH_SIZE)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n\nsample_batch = next(iter(train_loader))\nvideos, labels = sample_batch\nprint(f\"Videos batch shape: {videos.shape}\")\nprint(f\"Labels batch shape: {labels.shape}\")\nprint(f\"Labels: {labels}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:45:56.107442Z","iopub.execute_input":"2025-10-20T19:45:56.107882Z","iopub.status.idle":"2025-10-20T19:45:56.404024Z","shell.execute_reply.started":"2025-10-20T19:45:56.107856Z","shell.execute_reply":"2025-10-20T19:45:56.403088Z"}},"outputs":[{"name":"stdout","text":"Batch size set to: 2\nVideos batch shape: torch.Size([2, 32, 3, 224, 224])\nLabels batch shape: torch.Size([2])\nLabels: tensor([ 8, 11])\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## Model\n\nBegin with simple, straigh-forward version","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SlowFastModel(nn.Module):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.num_classes = num_classes\n\n        # Slow path: low frame rate, extracts context\n        self.slow_path = nn.Sequential(\n            nn.Conv3d(3, 32, kernel_size=3, stride=(1,2,2), padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool3d((4, 14, 14))\n        )\n\n        # Fast path: high frame rate, extracts dynamics\n        self.fast_path = nn.Sequential(\n            nn.Conv3d(3, 8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool3d((4, 14, 14))\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear((32 + 8) * 4 * 14 * 14, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        # x: [B, T, C, H, W] -> [B, C, T, H, W]\n        x = x.permute(0, 2, 1, 3, 4)\n\n        # divide into slow and fast flows\n        slow = x[:, :, ::8, :, :]  # take 1/8 frames\n        fast = x                     # all frames\n\n        # Conv3d + AdaptiveAvgPool3d\n        slow_feat = self.slow_path(slow)\n        fast_feat = self.fast_path(fast)\n\n        # \"flattening\" tensors for fully con layer\n        slow_feat = slow_feat.flatten(start_dim=1)\n        fast_feat = fast_feat.flatten(start_dim=1)\n\n        out = torch.cat([slow_feat, fast_feat], dim=1)\n\n        return self.fc(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:46:21.202132Z","iopub.execute_input":"2025-10-20T19:46:21.202800Z","iopub.status.idle":"2025-10-20T19:46:21.212282Z","shell.execute_reply.started":"2025-10-20T19:46:21.202770Z","shell.execute_reply":"2025-10-20T19:46:21.211019Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"file:///kaggle/working/mlruns\")\nmlflow.set_experiment(\"b-chef-slowfast\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:46:22.986633Z","iopub.execute_input":"2025-10-20T19:46:22.987513Z","iopub.status.idle":"2025-10-20T19:46:22.996358Z","shell.execute_reply.started":"2025-10-20T19:46:22.987484Z","shell.execute_reply":"2025-10-20T19:46:22.995609Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='file:///kaggle/working/mlruns/704353735472346475', creation_time=1760983364287, experiment_id='704353735472346475', last_update_time=1760983364287, lifecycle_stage='active', name='b-chef-slowfast', tags={}>"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    loop = tqdm(dataloader, desc=f\"ğŸ§  Train Epoch {epoch+1}\", leave=False)\n    for inputs, labels in loop:\n        inputs = inputs.to(device)\n        labels = labels.long().to(device)\n\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, preds = outputs.max(1)\n        total += labels.size(0)\n        correct += preds.eq(labels).sum().item()\n\n    train_loss = running_loss / len(dataloader)\n    train_acc = 100 * correct / total\n\n    mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n    mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n\n    return train_loss, train_acc\n\n\n@torch.no_grad()\ndef validate(model, dataloader, criterion, device, epoch):\n    model.eval()\n    val_loss, correct, total = 0.0, 0, 0\n\n    loop = tqdm(dataloader, desc=f\"ğŸ” Val Epoch {epoch+1}\", leave=False)\n    for inputs, labels in loop:\n        inputs = inputs.to(device)\n        labels = labels.long().to(device)\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        val_loss += loss.item()\n        _, preds = outputs.max(1)\n        total += labels.size(0)\n        correct += preds.eq(labels).sum().item()\n\n    val_loss /= len(dataloader)\n    val_acc = 100 * correct / total\n\n    mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n    mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n\n    return val_loss, val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:46:23.138634Z","iopub.execute_input":"2025-10-20T19:46:23.139223Z","iopub.status.idle":"2025-10-20T19:46:23.149794Z","shell.execute_reply.started":"2025-10-20T19:46:23.139201Z","shell.execute_reply":"2025-10-20T19:46:23.149027Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"num_classes = 13\nmodel = SlowFastModel(num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nnum_epochs = 5\n\nwith mlflow.start_run(run_name=\"slowfast_v1\"):\n    mlflow.log_params({\n        \"num_classes\": num_classes,\n        \"lr\": 1e-4,\n        \"epochs\": num_epochs,\n        \"optimizer\": \"Adam\",\n        \"batch_size\": train_loader.batch_size\n    })\n    \n    best_val_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f\"\\nğŸŒ Epoch [{epoch+1}/{num_epochs}]\")\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n\n        print(f\"ğŸ“ˆ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"ğŸ§ª Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n            print(\"ğŸ’¾ Best model updated!\")\n\n    mlflow.log_metric(\"best_val_acc\", best_val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:46:23.866647Z","iopub.execute_input":"2025-10-20T19:46:23.866940Z","iopub.status.idle":"2025-10-20T20:23:16.554121Z","shell.execute_reply.started":"2025-10-20T19:46:23.866919Z","shell.execute_reply":"2025-10-20T20:23:16.551348Z"}},"outputs":[{"name":"stdout","text":"\nğŸŒ Epoch [1/5]\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 19:54:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“ˆ Train Loss: 2.5263 | Train Acc: 15.50%\nğŸ§ª Val   Loss: 2.1605 | Val   Acc: 26.32%\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 19:54:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/10/20 19:54:40 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/10/20 19:54:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"ğŸ’¾ Best model updated!\n\nğŸŒ Epoch [2/5]\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 20:01:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n2025/10/20 20:01:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“ˆ Train Loss: 2.1780 | Train Acc: 27.03%\nğŸ§ª Val   Loss: 1.9477 | Val   Acc: 39.47%\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 20:01:38 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/10/20 20:01:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"ğŸ’¾ Best model updated!\n\nğŸŒ Epoch [3/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“ˆ Train Loss: 1.9240 | Train Acc: 37.24%\nğŸ§ª Val   Loss: 1.8280 | Val   Acc: 37.72%\n\nğŸŒ Epoch [4/5]\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 20:15:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“ˆ Train Loss: 1.7931 | Train Acc: 39.51%\nğŸ§ª Val   Loss: 1.7353 | Val   Acc: 47.37%\n","output_type":"stream"},{"name":"stderr","text":"2025/10/20 20:15:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/10/20 20:15:58 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/10/20 20:15:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"ğŸ’¾ Best model updated!\n\nğŸŒ Epoch [5/5]\n","output_type":"stream"},{"name":"stderr","text":"                                                                   ","output_type":"stream"},{"name":"stdout","text":"ğŸ“ˆ Train Loss: 1.6951 | Train Acc: 44.42%\nğŸ§ª Val   Loss: 1.6936 | Val   Acc: 42.98%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}